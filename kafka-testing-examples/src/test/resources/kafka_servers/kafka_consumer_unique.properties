# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
#          kafka consumer properties
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
group.id=consumerGroup_${RANDOM.NUMBER}
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
max.poll.records=2
enable.auto.commit=false
auto.offset.reset=earliest

#
# -----------------------------
# client.id is auto generated. Making it unique will have no effect if they belong to same group.
# Making the group.id as unique makes sense and the new group ca consume same records once again.
# client.id uniqueness will differentiate from another consumer in the same group.
# Refer : ConsumerConfig.java in the source code.
# /kafka/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java
# -----------------------------
#client.id=consumer-1123
#group.id=None
#enable.auto.commit=true
#key.deserializer=org.apache.kafka.common.serialization.LongDeserializer
#value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
#
## fast session timeout makes it more fun to play with failover
#session.timeout.ms=10000
#
## These buffer sizes seem to be needed to avoid consumer switching to
## a mode where it processes one bufferful every 5 seconds with multiple
## timeouts along the way.  No idea why this happens.
#fetch.min.bytes=50000
#receive.buffer.bytes=262144
#max.partition.fetch.bytes=2097152