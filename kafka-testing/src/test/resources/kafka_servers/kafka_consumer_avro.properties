# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
#          kafka consumer properties
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
group.id=consumerGroup11
key.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer

schema.registry.url=http://localhost:8081
max.poll.records=2
enable.auto.commit=false
auto.offset.reset=earliest

#
#bootstrap.servers=localhost:9092
#group.id=None
#enable.auto.commit=true
#key.deserializer=org.apache.kafka.common.serialization.LongDeserializer
#value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
#
## fast session timeout makes it more fun to play with failover
#session.timeout.ms=10000
#
## These buffer sizes seem to be needed to avoid consumer switching to
## a mode where it processes one bufferful every 5 seconds with multiple
## timeouts along the way.  No idea why this happens.
#fetch.min.bytes=50000
#receive.buffer.bytes=262144
#max.partition.fetch.bytes=2097152